import pandas as pd
import numpy as np
import sklearn
from collections import Counter
from sklearn.linear_model import Lasso
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score,cross_val_predict
from sklearn.model_selection import GridSearchCV
from sklearn.feature_selection import SelectFromModel
import re
from tqdm import tqdm # show progress bar

def get_model(X, Y, random_state_model=0):
    space = {'max_features': list(range(1,16))} 
    cv_outer = KFold(n_splits=10, random_state=0, shuffle=True)
    outer_results = list()
    inner_results = list()
    best_estimators = list()
    best_params = list()
    importances_per_fold = list()
    selected_features = list()
    for train_ix, test_ix in cv_outer.split(X):
        X_train, X_test = X.iloc[train_ix, :], X.iloc[test_ix, :]
        y_train, y_test = Y.iloc[train_ix], Y.iloc[test_ix]
        cv_inner = KFold(n_splits=5, random_state=0, shuffle=True)
        model = RandomForestRegressor(random_state=random_state_model)
        # define search
        search = GridSearchCV(model, space, scoring='r2', cv=cv_inner, refit=True)
        # execute search
        result = search.fit(X_train, y_train)
        # get the best performing model fit on the whole training set
        best_model = result.best_estimator_
        #save the best model
        best_estimators.append(best_model)
        #save the best parameters
        best_params.append(result.best_params_)
        #save the feature importances
        importances_per_fold.append(best_model.feature_importances_)
        #best_score_: Mean cross-validated score of the best_estimator
        inner_results.append(result.best_score_)
        #select the best features depending on the best model and feature importance
        best_features = list(X.columns[SelectFromModel(best_model, max_features = result.best_params_['max_features'], prefit=True).get_support()])
        selected_features.append(best_features)
        # evaluate model on the hold out dataset, get the R2 of the best_estimator
        yhat = best_model.predict(X_test)
        # evaluate the model, R2
        r2 = sklearn.metrics.r2_score(y_test, yhat)
        # store the result
        outer_results.append(r2)
        # report progress
        print('>R2=%.3f, est=%.3f, cfg=%s' % (r2, result.best_score_, result.best_params_))
    # summarize the estimated performance of the model
    print('inner R2: %.3f (%.3f)' % (np.mean(inner_results), np.std(inner_results)))
    print('outer R2: %.3f (%.3f)' % (np.mean(outer_results), np.std(outer_results)))
    #save the average feature importances
    importances = np.mean(importances_per_fold, axis=0)
    
    return [inner_results, outer_results, importances, best_estimators, best_params, selected_features]

def get_lasso_model(X, Y, random_state_model=0):
    space = {'alpha': [0.1, 0.5, 1, 2, 3, 4, 5, 10]}
    cv_outer = KFold(n_splits=10, random_state=0, shuffle=True)
    outer_results = list()
    inner_results = list()
    best_estimators = list()
    importances_per_fold = list()
    for train_ix, test_ix in cv_outer.split(X):
        X_train, X_test = X.iloc[train_ix, :], X.iloc[test_ix, :]
        y_train, y_test = Y.iloc[train_ix], Y.iloc[test_ix]
        cv_inner = KFold(n_splits=5, random_state=0, shuffle=True)
        model = Lasso(random_state=random_state_model)
        # define search
        search = GridSearchCV(model, space, scoring='r2', cv=cv_inner, refit=True)
        # execute search
        result = search.fit(X_train, y_train)
        # get the best performing model fit on the whole training set
        best_model = result.best_estimator_
        #save the best model
        best_estimators.append(best_model)
        #save the feature importances
        importances_per_fold.append(best_model.coef_)
        #best_score_: Mean cross-validated score of the best_estimator
        inner_results.append(result.best_score_)
        # evaluate model on the hold out dataset, get the R2 of the best_estimator
        yhat = best_model.predict(X_test)
        # evaluate the model, R2
        r2 = sklearn.metrics.r2_score(y_test, yhat)
        # store the result
        outer_results.append(r2)
        # report progress
        print('>R2=%.3f, est=%.3f, cfg=%s' % (r2, result.best_score_, result.best_params_))
    # summarize the estimated performance of the model
    print('inner R2: %.3f (%.3f)' % (np.mean(inner_results), np.std(inner_results)))
    print('outer R2: %.3f (%.3f)' % (np.mean(outer_results), np.std(outer_results)))
    #save the average feature importances
    importances = np.mean(importances_per_fold, axis=0)
    
    return [inner_results, outer_results, importances, best_estimators]
